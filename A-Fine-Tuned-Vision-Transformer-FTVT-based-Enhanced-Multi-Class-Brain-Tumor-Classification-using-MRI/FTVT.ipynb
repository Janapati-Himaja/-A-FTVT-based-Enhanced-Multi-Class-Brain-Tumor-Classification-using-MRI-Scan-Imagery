{"cells":[{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T09:35:22.831829Z","iopub.status.busy":"2024-02-21T09:35:22.830897Z","iopub.status.idle":"2024-02-21T09:35:22.835820Z","shell.execute_reply":"2024-02-21T09:35:22.834958Z","shell.execute_reply.started":"2024-02-21T09:35:22.831794Z"},"trusted":true},"outputs":[],"source":["import sys\n","sys.path.append(\"/kaggle/input/going-modular/going_modular\")  "]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T09:35:26.358203Z","iopub.status.busy":"2024-02-21T09:35:26.357352Z","iopub.status.idle":"2024-02-21T09:35:27.937696Z","shell.execute_reply":"2024-02-21T09:35:27.936748Z","shell.execute_reply.started":"2024-02-21T09:35:26.358170Z"},"trusted":true},"outputs":[],"source":["import engine\n"]},{"cell_type":"markdown","metadata":{},"source":["> > **fine tuned**"]},{"cell_type":"markdown","metadata":{},"source":["# 1 **FTVT_B 16**"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:28:22.580597Z","iopub.status.busy":"2024-02-11T18:28:22.580045Z","iopub.status.idle":"2024-02-11T18:28:26.415670Z","shell.execute_reply":"2024-02-11T18:28:26.414844Z","shell.execute_reply.started":"2024-02-11T18:28:22.580564Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","100%|██████████| 330M/330M [00:01<00:00, 243MB/s]  \n"]}],"source":["import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","from torch import nn\n","from torchvision import transforms\n","from torchinfo import summary\n","import os\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","# Set the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def set_seeds(seed: int = 42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","# 1. Get pretrained weights for ViT-Base\n","pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT \n","\n","# 2. Setup a ViT model instance with pretrained weights\n","pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n","\n","# 3. Freeze the base parameters\n","for parameter in pretrained_vit.parameters():\n","    parameter.requires_grad = False\n","\n","# 4. Modify the classifier head with BatchNormalization and Dense layers\n","class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n","\n","set_seeds()\n","# Modify the classifier head\n","pretrained_vit.heads = nn.Sequential(\n","    nn.BatchNorm1d(768),\n","    nn.Linear(in_features=768, out_features=512),\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=512, out_features=256),  # Experiment with different values\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=256, out_features=len(class_names))\n",").to(device)\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:28:26.417019Z","iopub.status.busy":"2024-02-11T18:28:26.416751Z","iopub.status.idle":"2024-02-11T18:28:28.067437Z","shell.execute_reply":"2024-02-11T18:28:28.066463Z","shell.execute_reply.started":"2024-02-11T18:28:26.416996Z"},"trusted":true},"outputs":[{"data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n","============================================================================================================================================\n","VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 4]              768                  Partial\n","├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n","├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n","│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n","│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n","│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n","│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n","├─Sequential (heads)                                         [32, 768]            [32, 4]              --                   True\n","│    └─BatchNorm1d (0)                                       [32, 768]            [32, 768]            1,536                True\n","│    └─Linear (1)                                            [32, 768]            [32, 512]            393,728              True\n","│    └─ReLU (2)                                              [32, 512]            [32, 512]            --                   --\n","│    └─Dropout (3)                                           [32, 512]            [32, 512]            --                   --\n","│    └─Linear (4)                                            [32, 512]            [32, 256]            131,328              True\n","│    └─ReLU (5)                                              [32, 256]            [32, 256]            --                   --\n","│    └─Dropout (6)                                           [32, 256]            [32, 256]            --                   --\n","│    └─Linear (7)                                            [32, 256]            [32, 4]              1,028                True\n","============================================================================================================================================\n","Total params: 86,326,276\n","Trainable params: 527,620\n","Non-trainable params: 85,798,656\n","Total mult-adds (G): 5.54\n","============================================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 3331.13\n","Params size (MB): 231.30\n","Estimated Total Size (MB): 3581.70\n","============================================================================================================================================"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# 5. Check the modified model summary\n","summary(model=pretrained_vit, \n","        input_size=(32, 3, 224, 224),  # Adjust the input size according to your data\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:28:28.069242Z","iopub.status.busy":"2024-02-11T18:28:28.068872Z","iopub.status.idle":"2024-02-11T18:28:28.421853Z","shell.execute_reply":"2024-02-11T18:28:28.420823Z","shell.execute_reply.started":"2024-02-11T18:28:28.069207Z"},"trusted":true},"outputs":[],"source":["# Setup directory paths to train and test images\n","train_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n","test_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n","\n","# 6. Get automatic transforms from pretrained ViT weights\n","pretrained_vit_transforms = pretrained_vit_weights.transforms()\n","\n","# 7. Create dataloaders with modified ViT model\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(\n","    train_dir: str, \n","    test_dir: str, \n","    transform: transforms.Compose, \n","    batch_size: int, \n","    num_workers: int=NUM_WORKERS\n","):\n","\n","  # Use ImageFolder to create dataset(s)\n","  train_data = datasets.ImageFolder(train_dir, transform=transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names\n","\n","# 8. Setup dataloaders with modified ViT model\n","train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n","                                                                                                     test_dir=test_dir,\n","                                                                                                     transform=pretrained_vit_transforms,\n","                                                                                                     batch_size=16)\n","\n","# The rest of the code remains the same for training the modified model"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:28:28.423216Z","iopub.status.busy":"2024-02-11T18:28:28.422936Z","iopub.status.idle":"2024-02-11T18:33:46.344448Z","shell.execute_reply":"2024-02-11T18:33:46.343341Z","shell.execute_reply.started":"2024-02-11T18:28:28.423192Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"878fa08a25744ab8b5bc898134cea0ad","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 1.1573 | train_acc: 0.5387 | test_loss: 0.8272 | test_acc: 0.7545\n","Epoch: 2 | train_loss: 0.7526 | train_acc: 0.7301 | test_loss: 0.5241 | test_acc: 0.8398\n","Epoch: 3 | train_loss: 0.5569 | train_acc: 0.8093 | test_loss: 0.3765 | test_acc: 0.8795\n","Epoch: 4 | train_loss: 0.4249 | train_acc: 0.8506 | test_loss: 0.2877 | test_acc: 0.9138\n","Epoch: 5 | train_loss: 0.3631 | train_acc: 0.8718 | test_loss: 0.2288 | test_acc: 0.9298\n","Epoch: 6 | train_loss: 0.2938 | train_acc: 0.9024 | test_loss: 0.1846 | test_acc: 0.9404\n","Epoch: 7 | train_loss: 0.2642 | train_acc: 0.9070 | test_loss: 0.1490 | test_acc: 0.9595\n","Epoch: 8 | train_loss: 0.2425 | train_acc: 0.9152 | test_loss: 0.1229 | test_acc: 0.9665\n","Epoch: 9 | train_loss: 0.2020 | train_acc: 0.9276 | test_loss: 0.1019 | test_acc: 0.9756\n","Epoch: 10 | train_loss: 0.1963 | train_acc: 0.9397 | test_loss: 0.0820 | test_acc: 0.9809\n"]}],"source":["\n","\n","# Create optimizer and loss function\n","optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n","                             lr=1e-4)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","\n","# Train the classifier head of the pretrained ViT feature extractor model\n","set_seeds()\n","pretrained_vit_results = engine.train(model=pretrained_vit,\n","                                      train_dataloader=train_dataloader_pretrained,\n","                                      test_dataloader=test_dataloader_pretrained,\n","                                      optimizer=optimizer,\n","                                      loss_fn=loss_fn,\n","                                      epochs=10,\n","                                      device=device)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:33:46.346190Z","iopub.status.busy":"2024-02-11T18:33:46.345882Z","iopub.status.idle":"2024-02-11T18:34:03.271457Z","shell.execute_reply":"2024-02-11T18:34:03.270184Z","shell.execute_reply.started":"2024-02-11T18:33:46.346159Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9809305873379099\n","F1 Score: 0.9809502294078271\n","Precision: 0.9810757276315637\n","Recall: 0.9809305873379099\n","Confusion Matrix:\n"," [[289   9   1   1]\n"," [  3 298   1   4]\n"," [  0   2 403   0]\n"," [  1   3   0 296]]\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","# Evaluate the model on the testing dataset\n","def evaluate_model(model, test_dataloader):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for images, labels in test_dataloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","    return y_true, y_pred\n","\n","# Evaluate the model and get the true and predicted labels\n","y_true, y_pred = evaluate_model(pretrained_vit, test_dataloader_pretrained)\n","\n","# Calculate metrics\n","accuracy = accuracy_score(y_true, y_pred)\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","# Calculate confusion matrix\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","\n","# Print metrics and confusion matrix\n","print(\"Accuracy:\", accuracy)\n","print(\"F1 Score:\", f1)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Confusion Matrix:\\n\", conf_matrix)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:34:03.277344Z","iopub.status.busy":"2024-02-11T18:34:03.276987Z","iopub.status.idle":"2024-02-11T18:34:19.601736Z","shell.execute_reply":"2024-02-11T18:34:19.600594Z","shell.execute_reply.started":"2024-02-11T18:34:03.277311Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","      glioma     0.9863    0.9633    0.9747       300\n","  meningioma     0.9551    0.9739    0.9644       306\n","     notumor     0.9951    0.9951    0.9951       405\n","   pituitary     0.9834    0.9867    0.9850       300\n","\n","    accuracy                         0.9809      1311\n","   macro avg     0.9800    0.9797    0.9798      1311\n","weighted avg     0.9811    0.9809    0.9810      1311\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","# After training the model, make predictions on the test dataset\n","pretrained_vit.eval()\n","predictions = []\n","targets = []\n","\n","with torch.no_grad():\n","    for images, labels in test_dataloader_pretrained:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pretrained_vit(images)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        targets.extend(labels.cpu().numpy())\n","\n","# Calculate classification report\n","report = classification_report(targets, predictions, target_names=class_names, digits=4)\n","print(report)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:34:19.603962Z","iopub.status.busy":"2024-02-11T18:34:19.603624Z","iopub.status.idle":"2024-02-11T18:34:36.008036Z","shell.execute_reply":"2024-02-11T18:34:36.006779Z","shell.execute_reply.started":"2024-02-11T18:34:19.603927Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overall Accuracy: 0.9809305873379099\n","Accuracy for class 'glioma': 0.988558352402746\n","Number of Misclassified Images in class 'glioma': 11/300\n","Accuracy for class 'meningioma': 0.9832189168573608\n","Number of Misclassified Images in class 'meningioma': 8/306\n","Accuracy for class 'notumor': 0.9969488939740656\n","Number of Misclassified Images in class 'notumor': 2/405\n","Accuracy for class 'pituitary': 0.9931350114416476\n","Number of Misclassified Images in class 'pituitary': 4/300\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","# After training the model, make predictions on the test dataset\n","pretrained_vit.eval()\n","predictions = []\n","targets = []\n","\n","# Store misclassified images\n","misclassified_images = []\n","\n","with torch.no_grad():\n","    for images, labels in test_dataloader_pretrained:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pretrained_vit(images)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        targets.extend(labels.cpu().numpy())\n","        \n","        # Check for misclassified images\n","        misclassified_indices = (predicted != labels).nonzero()\n","        misclassified_images.extend([images[idx].cpu() for idx in misclassified_indices])\n","\n","# Convert targets to integers\n","targets_int = [int(target) for target in targets]\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(targets_int, predictions)\n","print(\"Overall Accuracy:\", accuracy)\n","\n","# Calculate test size for each class\n","test_size_per_class = [sum(1 for target in targets_int if target == i) for i in range(len(class_names))]\n","\n","# Print misclassification information for each class\n","for i, class_name in enumerate(class_names):\n","    num_misclassified_class = sum(1 for label, prediction in zip(targets_int, predictions) if label == i and label != prediction)\n","    print(f\"Accuracy for class '{class_name}': {accuracy_score([1 if target == i else 0 for target in targets_int], [1 if prediction == i else 0 for prediction in predictions])}\")\n","    print(f\"Number of Misclassified Images in class '{class_name}': {num_misclassified_class}/{test_size_per_class[i]}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# 2 **FTVT_B_32**"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:34:36.010243Z","iopub.status.busy":"2024-02-11T18:34:36.009902Z","iopub.status.idle":"2024-02-11T18:34:39.040752Z","shell.execute_reply":"2024-02-11T18:34:39.039693Z","shell.execute_reply.started":"2024-02-11T18:34:36.010211Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vit_b_32-d86f8d99.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_32-d86f8d99.pth\n","100%|██████████| 337M/337M [00:01<00:00, 245MB/s]  \n"]}],"source":["import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","from torch import nn\n","from torchvision import transforms\n","from torchinfo import summary\n","import os\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","# Set the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def set_seeds(seed: int = 42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","# 1. Get pretrained weights for ViT-Base\n","pretrained_vit_weights = torchvision.models.ViT_B_32_Weights.DEFAULT \n","\n","# 2. Setup a ViT model instance with pretrained weights\n","pretrained_vit = torchvision.models.vit_b_32(weights=pretrained_vit_weights).to(device)\n","\n","# 3. Freeze the base parameters\n","for parameter in pretrained_vit.parameters():\n","    parameter.requires_grad = False\n","\n","# 4. Modify the classifier head with BatchNormalization and Dense layers\n","class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n","\n","set_seeds()\n","# Modify the classifier head with more layers or nodes\n","pretrained_vit.heads = nn.Sequential(\n","    nn.BatchNorm1d(768),\n","    nn.Linear(in_features=768, out_features=512),\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=512, out_features=256),  # Experiment with different values\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=256, out_features=len(class_names))\n",").to(device)\n","\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:34:39.042462Z","iopub.status.busy":"2024-02-11T18:34:39.042062Z","iopub.status.idle":"2024-02-11T18:34:39.183190Z","shell.execute_reply":"2024-02-11T18:34:39.182299Z","shell.execute_reply.started":"2024-02-11T18:34:39.042428Z"},"trusted":true},"outputs":[{"data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n","============================================================================================================================================\n","VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 4]              768                  Partial\n","├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 7, 7]      (2,360,064)          False\n","├─Encoder (encoder)                                          [32, 50, 768]        [32, 50, 768]        38,400               False\n","│    └─Dropout (dropout)                                     [32, 50, 768]        [32, 50, 768]        --                   --\n","│    └─Sequential (layers)                                   [32, 50, 768]        [32, 50, 768]        --                   False\n","│    │    └─EncoderBlock (encoder_layer_0)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_1)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_2)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_3)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_4)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_5)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_6)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_7)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_8)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_9)                   [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_10)                  [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    │    └─EncoderBlock (encoder_layer_11)                  [32, 50, 768]        [32, 50, 768]        (7,087,872)          False\n","│    └─LayerNorm (ln)                                        [32, 50, 768]        [32, 50, 768]        (1,536)              False\n","├─Sequential (heads)                                         [32, 768]            [32, 4]              --                   True\n","│    └─BatchNorm1d (0)                                       [32, 768]            [32, 768]            1,536                True\n","│    └─Linear (1)                                            [32, 768]            [32, 512]            393,728              True\n","│    └─ReLU (2)                                              [32, 512]            [32, 512]            --                   --\n","│    └─Dropout (3)                                           [32, 512]            [32, 512]            --                   --\n","│    └─Linear (4)                                            [32, 512]            [32, 256]            131,328              True\n","│    └─ReLU (5)                                              [32, 256]            [32, 256]            --                   --\n","│    └─Dropout (6)                                           [32, 256]            [32, 256]            --                   --\n","│    └─Linear (7)                                            [32, 256]            [32, 4]              1,028                True\n","============================================================================================================================================\n","Total params: 87,982,852\n","Trainable params: 527,620\n","Non-trainable params: 87,455,232\n","Total mult-adds (G): 5.53\n","============================================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 845.61\n","Params size (MB): 238.38\n","Estimated Total Size (MB): 1103.26\n","============================================================================================================================================"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Assuming input size is (3, 224, 224), adjust it based on your actual input size\n","#print(pretrained_vit)\n","# 5. Check the modified model summary\n","summary(model=pretrained_vit, \n","        input_size=(32, 3, 224, 224),  # Adjust the input size according to your data\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:34:39.184720Z","iopub.status.busy":"2024-02-11T18:34:39.184448Z","iopub.status.idle":"2024-02-11T18:34:39.215989Z","shell.execute_reply":"2024-02-11T18:34:39.215104Z","shell.execute_reply.started":"2024-02-11T18:34:39.184696Z"},"trusted":true},"outputs":[],"source":["# Setup directory paths to train and test images\n","train_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n","test_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n","\n","# 6. Get automatic transforms from pretrained ViT weights\n","pretrained_vit_transforms = pretrained_vit_weights.transforms()\n","\n","# 7. Create dataloaders with modified ViT model\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(\n","    train_dir: str, \n","    test_dir: str, \n","    transform: transforms.Compose, \n","    batch_size: int, \n","    num_workers: int=NUM_WORKERS\n","):\n","\n","  # Use ImageFolder to create dataset(s)\n","  train_data = datasets.ImageFolder(train_dir, transform=transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names\n","\n","# 8. Setup dataloaders with modified ViT model\n","train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n","                                                                                                     test_dir=test_dir,\n","                                                                                                     transform=pretrained_vit_transforms,\n","                                                                                                     batch_size=16)\n","\n","# The rest of the code remains the same for training the modified model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:34:39.217349Z","iopub.status.busy":"2024-02-11T18:34:39.217052Z","iopub.status.idle":"2024-02-11T18:36:17.093233Z","shell.execute_reply":"2024-02-11T18:36:17.092130Z","shell.execute_reply.started":"2024-02-11T18:34:39.217319Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"efb47c37731a444ebe5e638843f2d1b2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 1.1332 | train_acc: 0.5684 | test_loss: 0.8166 | test_acc: 0.7667\n","Epoch: 2 | train_loss: 0.7624 | train_acc: 0.7301 | test_loss: 0.5450 | test_acc: 0.8269\n","Epoch: 3 | train_loss: 0.5879 | train_acc: 0.7979 | test_loss: 0.4127 | test_acc: 0.8528\n","Epoch: 4 | train_loss: 0.4625 | train_acc: 0.8330 | test_loss: 0.3266 | test_acc: 0.8947\n","Epoch: 5 | train_loss: 0.4242 | train_acc: 0.8458 | test_loss: 0.2730 | test_acc: 0.9115\n","Epoch: 6 | train_loss: 0.3444 | train_acc: 0.8780 | test_loss: 0.2295 | test_acc: 0.9275\n","Epoch: 7 | train_loss: 0.3141 | train_acc: 0.8810 | test_loss: 0.1889 | test_acc: 0.9435\n","Epoch: 8 | train_loss: 0.2903 | train_acc: 0.8955 | test_loss: 0.1602 | test_acc: 0.9557\n","Epoch: 9 | train_loss: 0.2487 | train_acc: 0.9116 | test_loss: 0.1360 | test_acc: 0.9649\n","Epoch: 10 | train_loss: 0.2446 | train_acc: 0.9138 | test_loss: 0.1181 | test_acc: 0.9687\n"]}],"source":["\n","# Create optimizer and loss function\n","optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n","                             lr=1e-4)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","\n","# Train the classifier head of the pretrained ViT feature extractor model\n","set_seeds()\n","pretrained_vit_results = engine.train(model=pretrained_vit,\n","                                      train_dataloader=train_dataloader_pretrained,\n","                                      test_dataloader=test_dataloader_pretrained,\n","                                      optimizer=optimizer,\n","                                      loss_fn=loss_fn,\n","                                      epochs=10,\n","                                      device=device)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:36:17.095512Z","iopub.status.busy":"2024-02-11T18:36:17.095144Z","iopub.status.idle":"2024-02-11T18:36:21.871779Z","shell.execute_reply":"2024-02-11T18:36:21.870644Z","shell.execute_reply.started":"2024-02-11T18:36:17.095477Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9687261632341724\n","F1 Score: 0.9688457096046929\n","Precision: 0.9697962923425875\n","Recall: 0.9687261632341724\n","Confusion Matrix:\n"," [[278  22   0   0]\n"," [  2 295   3   6]\n"," [  0   1 404   0]\n"," [  1   6   0 293]]\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","# Evaluate the model on the testing dataset\n","def evaluate_model(model, test_dataloader):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for images, labels in test_dataloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","    return y_true, y_pred\n","\n","# Evaluate the model and get the true and predicted labels\n","y_true, y_pred = evaluate_model(pretrained_vit, test_dataloader_pretrained)\n","\n","# Calculate metrics\n","accuracy = accuracy_score(y_true, y_pred)\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","# Calculate confusion matrix\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","\n","# Print metrics and confusion matrix\n","print(\"Accuracy:\", accuracy)\n","print(\"F1 Score:\", f1)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Confusion Matrix:\\n\", conf_matrix)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:36:21.874123Z","iopub.status.busy":"2024-02-11T18:36:21.873601Z","iopub.status.idle":"2024-02-11T18:36:26.646273Z","shell.execute_reply":"2024-02-11T18:36:26.643341Z","shell.execute_reply.started":"2024-02-11T18:36:21.874081Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","      glioma     0.9893    0.9267    0.9570       300\n","  meningioma     0.9105    0.9641    0.9365       306\n","     notumor     0.9926    0.9975    0.9951       405\n","   pituitary     0.9799    0.9767    0.9783       300\n","\n","    accuracy                         0.9687      1311\n","   macro avg     0.9681    0.9662    0.9667      1311\n","weighted avg     0.9698    0.9687    0.9688      1311\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","from sklearn.metrics import classification_report\n","\n","# After training the model, make predictions on the test dataset\n","pretrained_vit.eval()\n","predictions = []\n","targets = []\n","\n","with torch.no_grad():\n","    for images, labels in test_dataloader_pretrained:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pretrained_vit(images)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        targets.extend(labels.cpu().numpy())\n","\n","# Calculate classification report\n","report = classification_report(targets, predictions, target_names=class_names, digits=4)\n","print(report)\n"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:36:26.648030Z","iopub.status.busy":"2024-02-11T18:36:26.647731Z","iopub.status.idle":"2024-02-11T18:36:31.561958Z","shell.execute_reply":"2024-02-11T18:36:31.560861Z","shell.execute_reply.started":"2024-02-11T18:36:26.648001Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overall Accuracy: 0.9687261632341724\n","Accuracy for class 'glioma': 0.9809305873379099\n","Number of Misclassified Images in class 'glioma': 22/300\n","Accuracy for class 'meningioma': 0.969488939740656\n","Number of Misclassified Images in class 'meningioma': 11/306\n","Accuracy for class 'notumor': 0.9969488939740656\n","Number of Misclassified Images in class 'notumor': 1/405\n","Accuracy for class 'pituitary': 0.9900839054157132\n","Number of Misclassified Images in class 'pituitary': 7/300\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","# After training the model, make predictions on the test dataset\n","pretrained_vit.eval()\n","predictions = []\n","targets = []\n","\n","# Store misclassified images\n","misclassified_images = []\n","\n","with torch.no_grad():\n","    for images, labels in test_dataloader_pretrained:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pretrained_vit(images)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        targets.extend(labels.cpu().numpy())\n","        \n","        # Check for misclassified images\n","        misclassified_indices = (predicted != labels).nonzero()\n","        misclassified_images.extend([images[idx].cpu() for idx in misclassified_indices])\n","\n","# Convert targets to integers\n","targets_int = [int(target) for target in targets]\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(targets_int, predictions)\n","print(\"Overall Accuracy:\", accuracy)\n","\n","# Calculate test size for each class\n","test_size_per_class = [sum(1 for target in targets_int if target == i) for i in range(len(class_names))]\n","\n","# Print misclassification information for each class\n","for i, class_name in enumerate(class_names):\n","    num_misclassified_class = sum(1 for label, prediction in zip(targets_int, predictions) if label == i and label != prediction)\n","    print(f\"Accuracy for class '{class_name}': {accuracy_score([1 if target == i else 0 for target in targets_int], [1 if prediction == i else 0 for prediction in predictions])}\")\n","    print(f\"Number of Misclassified Images in class '{class_name}': {num_misclassified_class}/{test_size_per_class[i]}\")"]},{"cell_type":"markdown","metadata":{},"source":["# 3 FTVT_L_32"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:36:31.564548Z","iopub.status.busy":"2024-02-11T18:36:31.563781Z","iopub.status.idle":"2024-02-11T18:36:42.211782Z","shell.execute_reply":"2024-02-11T18:36:42.210819Z","shell.execute_reply.started":"2024-02-11T18:36:31.564497Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vit_l_32-c7638314.pth\" to /root/.cache/torch/hub/checkpoints/vit_l_32-c7638314.pth\n","100%|██████████| 1.14G/1.14G [00:05<00:00, 221MB/s] \n"]}],"source":["import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","from torch import nn\n","from torchvision import transforms\n","from torchinfo import summary\n","import os\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","# Set the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def set_seeds(seed: int = 42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","# 1. Get pretrained weights for ViT-Base\n","pretrained_vit_weights = torchvision.models.ViT_L_32_Weights.DEFAULT \n","\n","# 2. Setup a ViT model instance with pretrained weights\n","pretrained_vit = torchvision.models.vit_l_32(weights=pretrained_vit_weights).to(device)\n","\n","# 3. Freeze the base parameters\n","for parameter in pretrained_vit.parameters():\n","    parameter.requires_grad = False\n","\n","# 4. Modify the classifier head with BatchNormalization and Dense layers\n","class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n","\n","set_seeds()\n","# Modify the classifier head with more layers or nodes\n","pretrained_vit.heads = nn.Sequential(\n","    nn.BatchNorm1d(1024),\n","    nn.Linear(in_features=1024, out_features=512),  # Adjust input size to match the previous layer's output size\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=512, out_features=256),\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=256, out_features=len(class_names))\n",").to(device)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:36:42.213202Z","iopub.status.busy":"2024-02-11T18:36:42.212942Z","iopub.status.idle":"2024-02-11T18:36:42.485378Z","shell.execute_reply":"2024-02-11T18:36:42.484489Z","shell.execute_reply.started":"2024-02-11T18:36:42.213179Z"},"trusted":true},"outputs":[{"data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n","============================================================================================================================================\n","VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 4]              1,024                Partial\n","├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 1024, 7, 7]     (3,146,752)          False\n","├─Encoder (encoder)                                          [32, 50, 1024]       [32, 50, 1024]       51,200               False\n","│    └─Dropout (dropout)                                     [32, 50, 1024]       [32, 50, 1024]       --                   --\n","│    └─Sequential (layers)                                   [32, 50, 1024]       [32, 50, 1024]       --                   False\n","│    │    └─EncoderBlock (encoder_layer_0)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_1)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_2)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_3)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_4)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_5)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_6)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_7)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_8)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_9)                   [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_10)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_11)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_12)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_13)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_14)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_15)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_16)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_17)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_18)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_19)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_20)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_21)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_22)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_23)                  [32, 50, 1024]       [32, 50, 1024]       (12,596,224)         False\n","│    └─LayerNorm (ln)                                        [32, 50, 1024]       [32, 50, 1024]       (2,048)              False\n","├─Sequential (heads)                                         [32, 1024]           [32, 4]              --                   True\n","│    └─BatchNorm1d (0)                                       [32, 1024]           [32, 1024]           2,048                True\n","│    └─Linear (1)                                            [32, 1024]           [32, 512]            524,800              True\n","│    └─ReLU (2)                                              [32, 512]            [32, 512]            --                   --\n","│    └─Dropout (3)                                           [32, 512]            [32, 512]            --                   --\n","│    └─Linear (4)                                            [32, 512]            [32, 256]            131,328              True\n","│    └─ReLU (5)                                              [32, 256]            [32, 256]            --                   --\n","│    └─Dropout (6)                                           [32, 256]            [32, 256]            --                   --\n","│    └─Linear (7)                                            [32, 256]            [32, 4]              1,028                True\n","============================================================================================================================================\n","Total params: 306,169,604\n","Trainable params: 659,204\n","Non-trainable params: 305,510,400\n","Total mult-adds (G): 11.40\n","============================================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 2228.42\n","Params size (MB): 821.42\n","Estimated Total Size (MB): 3069.11\n","============================================================================================================================================"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Assuming input size is (3, 224, 224), adjust it based on your actual input size\n","#print(pretrained_vit)\n","# 5. Check the modified model summary\n","summary(model=pretrained_vit, \n","        input_size=(32, 3, 224, 224),  # Adjust the input size according to your data\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")\n"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:36:42.487158Z","iopub.status.busy":"2024-02-11T18:36:42.486766Z","iopub.status.idle":"2024-02-11T18:36:42.527661Z","shell.execute_reply":"2024-02-11T18:36:42.526763Z","shell.execute_reply.started":"2024-02-11T18:36:42.487123Z"},"trusted":true},"outputs":[],"source":["# Setup directory paths to train and test images\n","train_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n","test_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n","\n","# 6. Get automatic transforms from pretrained ViT weights\n","pretrained_vit_transforms = pretrained_vit_weights.transforms()\n","\n","# 7. Create dataloaders with modified ViT model\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(\n","    train_dir: str, \n","    test_dir: str, \n","    transform: transforms.Compose, \n","    batch_size: int, \n","    num_workers: int=NUM_WORKERS\n","):\n","\n","  # Use ImageFolder to create dataset(s)\n","  train_data = datasets.ImageFolder(train_dir, transform=transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names\n","\n","# 8. Setup dataloaders with modified ViT model\n","train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n","                                                                                                     test_dir=test_dir,\n","                                                                                                     transform=pretrained_vit_transforms,\n","                                                                                                     batch_size=16)\n","\n","# The rest of the code remains the same for training the modified model"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:36:42.529488Z","iopub.status.busy":"2024-02-11T18:36:42.529174Z","iopub.status.idle":"2024-02-11T18:41:52.304246Z","shell.execute_reply":"2024-02-11T18:41:52.303268Z","shell.execute_reply.started":"2024-02-11T18:36:42.529461Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"05eb4d6786224aa7b5dd7354b0d72775","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 1.1240 | train_acc: 0.5578 | test_loss: 0.8084 | test_acc: 0.7718\n","Epoch: 2 | train_loss: 0.7541 | train_acc: 0.7323 | test_loss: 0.5376 | test_acc: 0.8391\n","Epoch: 3 | train_loss: 0.5640 | train_acc: 0.8101 | test_loss: 0.3881 | test_acc: 0.8680\n","Epoch: 4 | train_loss: 0.4394 | train_acc: 0.8360 | test_loss: 0.3003 | test_acc: 0.8993\n","Epoch: 5 | train_loss: 0.3803 | train_acc: 0.8588 | test_loss: 0.2360 | test_acc: 0.9306\n","Epoch: 6 | train_loss: 0.3067 | train_acc: 0.9024 | test_loss: 0.1921 | test_acc: 0.9436\n","Epoch: 7 | train_loss: 0.2604 | train_acc: 0.9092 | test_loss: 0.1479 | test_acc: 0.9588\n","Epoch: 8 | train_loss: 0.2418 | train_acc: 0.9169 | test_loss: 0.1183 | test_acc: 0.9710\n","Epoch: 9 | train_loss: 0.2111 | train_acc: 0.9314 | test_loss: 0.0940 | test_acc: 0.9809\n","Epoch: 10 | train_loss: 0.1935 | train_acc: 0.9344 | test_loss: 0.0750 | test_acc: 0.9863\n"]}],"source":["\n","# Create optimizer and loss function\n","optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n","                             lr=1e-4)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","\n","# Train the classifier head of the pretrained ViT feature extractor model\n","set_seeds()\n","pretrained_vit_results = engine.train(model=pretrained_vit,\n","                                      train_dataloader=train_dataloader_pretrained,\n","                                      test_dataloader=test_dataloader_pretrained,\n","                                      optimizer=optimizer,\n","                                      loss_fn=loss_fn,\n","                                      epochs=10,\n","                                      device=device)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:41:52.306208Z","iopub.status.busy":"2024-02-11T18:41:52.305878Z","iopub.status.idle":"2024-02-11T18:42:07.676697Z","shell.execute_reply":"2024-02-11T18:42:07.675595Z","shell.execute_reply.started":"2024-02-11T18:41:52.306175Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9862700228832952\n","F1 Score: 0.986287309838429\n","Precision: 0.9863661338039442\n","Recall: 0.9862700228832952\n","Confusion Matrix:\n"," [[292   8   0   0]\n"," [  2 300   1   3]\n"," [  0   1 404   0]\n"," [  1   2   0 297]]\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","# Evaluate the model on the testing dataset\n","def evaluate_model(model, test_dataloader):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for images, labels in test_dataloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","    return y_true, y_pred\n","\n","# Evaluate the model and get the true and predicted labels\n","y_true, y_pred = evaluate_model(pretrained_vit, test_dataloader_pretrained)\n","\n","# Calculate metrics\n","accuracy = accuracy_score(y_true, y_pred)\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","# Calculate confusion matrix\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","\n","# Print metrics and confusion matrix\n","print(\"Accuracy:\", accuracy)\n","print(\"F1 Score:\", f1)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Confusion Matrix:\\n\", conf_matrix)"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:42:07.679250Z","iopub.status.busy":"2024-02-11T18:42:07.678517Z","iopub.status.idle":"2024-02-11T18:42:23.026413Z","shell.execute_reply":"2024-02-11T18:42:23.025309Z","shell.execute_reply.started":"2024-02-11T18:42:07.679207Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","      glioma     0.9898    0.9733    0.9815       300\n","  meningioma     0.9646    0.9804    0.9724       306\n","     notumor     0.9975    0.9975    0.9975       405\n","   pituitary     0.9900    0.9900    0.9900       300\n","\n","    accuracy                         0.9863      1311\n","   macro avg     0.9855    0.9853    0.9854      1311\n","weighted avg     0.9864    0.9863    0.9863      1311\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","# After training the model, make predictions on the test dataset\n","pretrained_vit.eval()\n","predictions = []\n","targets = []\n","\n","with torch.no_grad():\n","    for images, labels in test_dataloader_pretrained:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pretrained_vit(images)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        targets.extend(labels.cpu().numpy())\n","\n","# Calculate classification report\n","report = classification_report(targets, predictions, target_names=class_names, digits=4)\n","print(report)\n"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:42:23.031969Z","iopub.status.busy":"2024-02-11T18:42:23.031672Z","iopub.status.idle":"2024-02-11T18:42:38.376744Z","shell.execute_reply":"2024-02-11T18:42:38.375601Z","shell.execute_reply.started":"2024-02-11T18:42:23.031940Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overall Accuracy: 0.9862700228832952\n","Accuracy for class 'glioma': 0.9916094584286804\n","Number of Misclassified Images in class 'glioma': 8/300\n","Accuracy for class 'meningioma': 0.9870327993897788\n","Number of Misclassified Images in class 'meningioma': 6/306\n","Accuracy for class 'notumor': 0.9984744469870328\n","Number of Misclassified Images in class 'notumor': 1/405\n","Accuracy for class 'pituitary': 0.9954233409610984\n","Number of Misclassified Images in class 'pituitary': 3/300\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","# After training the model, make predictions on the test dataset\n","pretrained_vit.eval()\n","predictions = []\n","targets = []\n","\n","# Store misclassified images\n","misclassified_images = []\n","\n","with torch.no_grad():\n","    for images, labels in test_dataloader_pretrained:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pretrained_vit(images)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        targets.extend(labels.cpu().numpy())\n","        \n","        # Check for misclassified images\n","        misclassified_indices = (predicted != labels).nonzero()\n","        misclassified_images.extend([images[idx].cpu() for idx in misclassified_indices])\n","\n","# Convert targets to integers\n","targets_int = [int(target) for target in targets]\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(targets_int, predictions)\n","print(\"Overall Accuracy:\", accuracy)\n","\n","# Calculate test size for each class\n","test_size_per_class = [sum(1 for target in targets_int if target == i) for i in range(len(class_names))]\n","\n","# Print misclassification information for each class\n","for i, class_name in enumerate(class_names):\n","    num_misclassified_class = sum(1 for label, prediction in zip(targets_int, predictions) if label == i and label != prediction)\n","    print(f\"Accuracy for class '{class_name}': {accuracy_score([1 if target == i else 0 for target in targets_int], [1 if prediction == i else 0 for prediction in predictions])}\")\n","    print(f\"Number of Misclassified Images in class '{class_name}': {num_misclassified_class}/{test_size_per_class[i]}\")"]},{"cell_type":"markdown","metadata":{},"source":["# 4 **FTVT_L_16**"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T09:35:36.262593Z","iopub.status.busy":"2024-02-21T09:35:36.261547Z","iopub.status.idle":"2024-02-21T09:35:48.066999Z","shell.execute_reply":"2024-02-21T09:35:48.065978Z","shell.execute_reply.started":"2024-02-21T09:35:36.262560Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vit_l_16-852ce7e3.pth\" to /root/.cache/torch/hub/checkpoints/vit_l_16-852ce7e3.pth\n","100%|██████████| 1.13G/1.13G [00:05<00:00, 220MB/s] \n"]}],"source":["import matplotlib.pyplot as plt\n","import torch\n","\n","import torchvision\n","from torch import nn\n","from torchvision import transforms\n","from torchinfo import summary\n","import os\n","from torchvision import datasets\n","from torch.utils.data import DataLoader\n","\n","# Set the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","def set_seeds(seed: int = 42):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","\n","# 1. Get pretrained weights for ViT-Base\n","pretrained_vit_weights = torchvision.models.ViT_L_16_Weights.DEFAULT \n","\n","# 2. Setup a ViT model instance with pretrained weights\n","pretrained_vit = torchvision.models.vit_l_16(weights=pretrained_vit_weights).to(device)\n","\n","# 3. Freeze the base parameters\n","for parameter in pretrained_vit.parameters():\n","    parameter.requires_grad = False\n","\n","# 4. Modify the classifier head with BatchNormalization and Dense layers\n","class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n","\n","set_seeds()\n","# Modify the classifier head with more layers or nodes\n","pretrained_vit.heads = nn.Sequential(\n","    nn.BatchNorm1d(1024),\n","    nn.Linear(in_features=1024, out_features=512),  # Adjust input size to match the previous layer's output size\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=512, out_features=256),\n","    nn.ReLU(),\n","    nn.Dropout(0.5),\n","    nn.Linear(in_features=256, out_features=len(class_names))\n",").to(device)\n"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T09:35:48.781540Z","iopub.status.busy":"2024-02-21T09:35:48.780912Z","iopub.status.idle":"2024-02-21T09:35:51.144472Z","shell.execute_reply":"2024-02-21T09:35:51.143472Z","shell.execute_reply.started":"2024-02-21T09:35:48.781506Z"},"trusted":true},"outputs":[{"data":{"text/plain":["============================================================================================================================================\n","Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n","============================================================================================================================================\n","VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 4]              1,024                Partial\n","├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 1024, 14, 14]   (787,456)            False\n","├─Encoder (encoder)                                          [32, 197, 1024]      [32, 197, 1024]      201,728              False\n","│    └─Dropout (dropout)                                     [32, 197, 1024]      [32, 197, 1024]      --                   --\n","│    └─Sequential (layers)                                   [32, 197, 1024]      [32, 197, 1024]      --                   False\n","│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_12)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_13)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_14)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_15)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_16)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_17)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_18)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_19)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_20)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_21)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_22)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    │    └─EncoderBlock (encoder_layer_23)                  [32, 197, 1024]      [32, 197, 1024]      (12,596,224)         False\n","│    └─LayerNorm (ln)                                        [32, 197, 1024]      [32, 197, 1024]      (2,048)              False\n","├─Sequential (heads)                                         [32, 1024]           [32, 4]              --                   True\n","│    └─BatchNorm1d (0)                                       [32, 1024]           [32, 1024]           2,048                True\n","│    └─Linear (1)                                            [32, 1024]           [32, 512]            524,800              True\n","│    └─ReLU (2)                                              [32, 512]            [32, 512]            --                   --\n","│    └─Dropout (3)                                           [32, 512]            [32, 512]            --                   --\n","│    └─Linear (4)                                            [32, 512]            [32, 256]            131,328              True\n","│    └─ReLU (5)                                              [32, 256]            [32, 256]            --                   --\n","│    └─Dropout (6)                                           [32, 256]            [32, 256]            --                   --\n","│    └─Linear (7)                                            [32, 256]            [32, 4]              1,028                True\n","============================================================================================================================================\n","Total params: 303,960,836\n","Trainable params: 659,204\n","Non-trainable params: 303,301,632\n","Total mult-adds (G): 11.41\n","============================================================================================================================================\n","Input size (MB): 19.27\n","Forward/backward pass size (MB): 8779.40\n","Params size (MB): 811.99\n","Estimated Total Size (MB): 9610.65\n","============================================================================================================================================"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["# Assuming input size is (3, 224, 224), adjust it based on your actual input size\n","#print(pretrained_vit)\n","# 5. Check the modified model summary\n","summary(model=pretrained_vit, \n","        input_size=(32, 3, 224, 224),  # Adjust the input size according to your data\n","        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n","        col_width=20,\n","        row_settings=[\"var_names\"]\n",")\n"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T09:35:55.232916Z","iopub.status.busy":"2024-02-21T09:35:55.232172Z","iopub.status.idle":"2024-02-21T09:35:55.920690Z","shell.execute_reply":"2024-02-21T09:35:55.919537Z","shell.execute_reply.started":"2024-02-21T09:35:55.232882Z"},"trusted":true},"outputs":[],"source":["# Setup directory paths to train and test images\n","train_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n","test_dir = '/kaggle/input/brain-tumor-mri-dataset/Testing'\n","\n","# 6. Get automatic transforms from pretrained ViT weights\n","pretrained_vit_transforms = pretrained_vit_weights.transforms()\n","\n","# 7. Create dataloaders with modified ViT model\n","NUM_WORKERS = os.cpu_count()\n","\n","def create_dataloaders(\n","    train_dir: str, \n","    test_dir: str, \n","    transform: transforms.Compose, \n","    batch_size: int, \n","    num_workers: int=NUM_WORKERS\n","):\n","\n","  # Use ImageFolder to create dataset(s)\n","  train_data = datasets.ImageFolder(train_dir, transform=transform)\n","  test_data = datasets.ImageFolder(test_dir, transform=transform)\n","\n","  # Get class names\n","  class_names = train_data.classes\n","\n","  # Turn images into data loaders\n","  train_dataloader = DataLoader(\n","      train_data,\n","      batch_size=batch_size,\n","      shuffle=True,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","  test_dataloader = DataLoader(\n","      test_data,\n","      batch_size=batch_size,\n","      shuffle=False,\n","      num_workers=num_workers,\n","      pin_memory=True,\n","  )\n","\n","  return train_dataloader, test_dataloader, class_names\n","\n","# 8. Setup dataloaders with modified ViT model\n","train_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n","                                                                                                     test_dir=test_dir,\n","                                                                                                     transform=pretrained_vit_transforms,\n","                                                                                                     batch_size=16)\n","\n","# The rest of the code remains the same for training the modified model\n"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T09:36:12.769767Z","iopub.status.busy":"2024-02-21T09:36:12.769091Z","iopub.status.idle":"2024-02-21T09:51:25.959082Z","shell.execute_reply":"2024-02-21T09:51:25.957973Z","shell.execute_reply.started":"2024-02-21T09:36:12.769732Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"692db8b7386549069ae58b326a35c8da","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/10 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 1.0826 | train_acc: 0.6111 | test_loss: 0.7311 | test_acc: 0.8034\n","Epoch: 2 | train_loss: 0.6639 | train_acc: 0.7659 | test_loss: 0.4291 | test_acc: 0.8825\n","Epoch: 3 | train_loss: 0.4629 | train_acc: 0.8514 | test_loss: 0.2944 | test_acc: 0.9138\n","Epoch: 4 | train_loss: 0.3564 | train_acc: 0.8871 | test_loss: 0.2173 | test_acc: 0.9351\n","Epoch: 5 | train_loss: 0.2985 | train_acc: 0.9062 | test_loss: 0.1678 | test_acc: 0.9526\n","Epoch: 6 | train_loss: 0.2337 | train_acc: 0.9215 | test_loss: 0.1335 | test_acc: 0.9657\n","Epoch: 7 | train_loss: 0.2131 | train_acc: 0.9199 | test_loss: 0.1074 | test_acc: 0.9710\n","Epoch: 8 | train_loss: 0.1870 | train_acc: 0.9352 | test_loss: 0.0873 | test_acc: 0.9763\n","Epoch: 9 | train_loss: 0.1688 | train_acc: 0.9512 | test_loss: 0.0646 | test_acc: 0.9847\n","Epoch: 10 | train_loss: 0.1563 | train_acc: 0.9550 | test_loss: 0.0532 | test_acc: 0.9870\n"]}],"source":["\n","# Create optimizer and loss function\n","optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), \n","                             lr=1e-4)\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","\n","# Train the classifier head of the pretrained ViT feature extractor model\n","set_seeds()\n","pretrained_vit_results = engine.train(model=pretrained_vit,\n","                                      train_dataloader=train_dataloader_pretrained,\n","                                      test_dataloader=test_dataloader_pretrained,\n","                                      optimizer=optimizer,\n","                                 \n","                                      loss_fn=loss_fn,\n","                                      epochs=10,\n","                                      device=device)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-02-21T09:54:09.776610Z","iopub.status.busy":"2024-02-21T09:54:09.775828Z","iopub.status.idle":"2024-02-21T09:54:56.472473Z","shell.execute_reply":"2024-02-21T09:54:56.471286Z","shell.execute_reply.started":"2024-02-21T09:54:09.776574Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.9870327993897788\n","F1 Score: 0.9870232372572859\n","Precision: 0.9870554527448101\n","Recall: 0.9870327993897788\n","Confusion Matrix:\n"," [[295   4   0   1]\n"," [  3 297   1   5]\n"," [  0   2 403   0]\n"," [  0   1   0 299]]\n"]}],"source":["from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","# Evaluate the model on the testing dataset\n","def evaluate_model(model, test_dataloader):\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    with torch.no_grad():\n","        for images, labels in test_dataloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","    return y_true, y_pred\n","\n","# Evaluate the model and get the true and predicted labels\n","y_true, y_pred = evaluate_model(pretrained_vit, test_dataloader_pretrained)\n","\n","# Calculate metrics\n","accuracy = accuracy_score(y_true, y_pred)\n","f1 = f1_score(y_true, y_pred, average='weighted')\n","precision = precision_score(y_true, y_pred, average='weighted')\n","recall = recall_score(y_true, y_pred, average='weighted')\n","\n","# Calculate confusion matrix\n","conf_matrix = confusion_matrix(y_true, y_pred)\n","\n","# Print metrics and confusion matrix\n","print(\"Accuracy:\", accuracy)\n","print(\"F1 Score:\", f1)\n","print(\"Precision:\", precision)\n","print(\"Recall:\", recall)\n","print(\"Confusion Matrix:\\n\", conf_matrix)"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T18:59:31.057377Z","iopub.status.busy":"2024-02-11T18:59:31.057031Z","iopub.status.idle":"2024-02-11T19:00:17.885635Z","shell.execute_reply":"2024-02-11T19:00:17.884572Z","shell.execute_reply.started":"2024-02-11T18:59:31.057346Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","      glioma     0.9899    0.9833    0.9866       300\n","  meningioma     0.9770    0.9706    0.9738       306\n","     notumor     0.9975    0.9951    0.9963       405\n","   pituitary     0.9803    0.9967    0.9884       300\n","\n","    accuracy                         0.9870      1311\n","   macro avg     0.9862    0.9864    0.9863      1311\n","weighted avg     0.9871    0.9870    0.9870      1311\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","# After training the model, make predictions on the test dataset\n","pretrained_vit.eval()\n","predictions = []\n","targets = []\n","\n","with torch.no_grad():\n","    for images, labels in test_dataloader_pretrained:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pretrained_vit(images)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        targets.extend(labels.cpu().numpy())\n","\n","# Calculate classification report\n","report = classification_report(targets, predictions, target_names=class_names, digits=4)\n","print(report)\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-02-11T19:00:17.887413Z","iopub.status.busy":"2024-02-11T19:00:17.887089Z","iopub.status.idle":"2024-02-11T19:01:04.704770Z","shell.execute_reply":"2024-02-11T19:01:04.703752Z","shell.execute_reply.started":"2024-02-11T19:00:17.887383Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overall Accuracy: 0.9870327993897788\n","Accuracy for class 'glioma': 0.9938977879481312\n","Number of Misclassified Images in class 'glioma': 5/300\n","Accuracy for class 'meningioma': 0.9877955758962624\n","Number of Misclassified Images in class 'meningioma': 9/306\n","Accuracy for class 'notumor': 0.9977116704805492\n","Number of Misclassified Images in class 'notumor': 2/405\n","Accuracy for class 'pituitary': 0.9946605644546148\n","Number of Misclassified Images in class 'pituitary': 1/300\n"]}],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","\n","# After training the model, make predictions on the test dataset\n","pretrained_vit.eval()\n","predictions = []\n","targets = []\n","\n","# Store misclassified images\n","misclassified_images = []\n","\n","with torch.no_grad():\n","    for images, labels in test_dataloader_pretrained:\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = pretrained_vit(images)\n","        _, predicted = torch.max(outputs, 1)\n","        predictions.extend(predicted.cpu().numpy())\n","        targets.extend(labels.cpu().numpy())\n","        \n","        # Check for misclassified images\n","        misclassified_indices = (predicted != labels).nonzero()\n","        misclassified_images.extend([images[idx].cpu() for idx in misclassified_indices])\n","\n","# Convert targets to integers\n","targets_int = [int(target) for target in targets]\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(targets_int, predictions)\n","print(\"Overall Accuracy:\", accuracy)\n","\n","# Calculate test size for each class\n","test_size_per_class = [sum(1 for target in targets_int if target == i) for i in range(len(class_names))]\n","\n","# Print misclassification information for each class\n","for i, class_name in enumerate(class_names):\n","    num_misclassified_class = sum(1 for label, prediction in zip(targets_int, predictions) if label == i and label != prediction)\n","    print(f\"Accuracy for class '{class_name}': {accuracy_score([1 if target == i else 0 for target in targets_int], [1 if prediction == i else 0 for prediction in predictions])}\")\n","    print(f\"Number of Misclassified Images in class '{class_name}': {num_misclassified_class}/{test_size_per_class[i]}\")"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":1608934,"sourceId":2645886,"sourceType":"datasetVersion"},{"datasetId":4342994,"sourceId":7461934,"sourceType":"datasetVersion"},{"datasetId":4420238,"sourceId":7594132,"sourceType":"datasetVersion"}],"dockerImageVersionId":30636,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
